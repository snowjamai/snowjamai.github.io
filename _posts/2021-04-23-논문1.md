---
title: "ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst 논문 Review"

categories:
- 논문

tags:
- 논문
- 딥러닝
- 자율주행

use_math: true
comments: true
---

***

### 1. Introduction
원하는 목적지까지 도로법을 따르며 안전하게 차량을 주행하기 위해서는 운전자가 주변 환경의 다양한 물체를 보고 이해하고 미래 행동과 상호작용에 대해 예측하고 자동차의 제어 방법을 계획해야 한다. 이러한 계획법은 기존 robotics에서는 어려운 과제였지만 Imitation Learning은 이것에 대한 가능성있는 접근이 될 수 있다. 해당 논문은 실제 주행까지 가능한 수준의 모방 학습방법에 관한 논문이다. 

학습 데이터로는 실제 숙련된 운전자가 60일동안 운전한 3000만 데이터를 사용하였다. camera 및 lidar 입력을 받아 운전대 각도나 브레이크 밟는 세기를 학습하는 End-to-End Learning쪽으로는 굉장히 많은 insight가 존재하지만 __sample complexity__(loss function을 성공적으로 학습하기 위해 필요한 sample의 수)를 줄이기 위해 Mid-to-Mid Learning을 진행하였다. Mid-to-Mid Learning을 위하여 raw 센서 데이터를 top-down 방식의 환경 및 주행 의도, 차량의 정보 및 도로 정보(3. 모델 구조에서 자세이 설명)등의 data로 변환하여 사용하였다. 이러한 입력들을 ChauffeurNet이라 하는 RNN에 넣어 조향과 브레이크를 위한 주행 경로를 예측값으로 출력한다. 이러한 Mid-to-Mid Learning의 장점으로는 실제 데이터나 시뮬레이션 데이터 모두 학습 가능하며, 실제 차에서 실행하기 전에 closed-loop simulation을 통해 테스트가 가능하다는 장점이있다.

학습을위한 3000만개의 샘플조차 input을 변환하여 넣어주었더라도 모방학습을 시키기에는 데이터가 부족하였다. 예를들어 학습이 완료된 모델임에도 불구하고, 좁은길가에 차가 주차되어있다면 주행차가 이를 피해가지 못하고 멈추는 경우가 발생하였다. 핵심적인 문제는 Closed-loop 시스템에서 실행할 때, error가 누적되어 training distribution이 이동한다는 것이다. 이러한 결과에 비추어 볼때, pure imitation Learning은 운전 영역에 대한 한계가 존재한다 볼 수 있다. 이러한 문제들이 존재하므로 실제적으로 해당 문제들을 직접 노출시키는 방법이 아닌 다른 방법이 필요하였다.

주행경로에 대한 작은 변화를 주고 합성하여 data augmentation을 통해 안좋은 action에 대한 억제와 발전에 대한 encourage로 loss를 수정함으로서 직접 상황에 노출시키지 않고 학습하는데 성공하였다. 즉, 이러한 perturbation(작은 변화)를 통한 비숙련자적 data(충돌 상황이나 도로를 벗어나는 상황)에 대한 노출이 해당 상황을 피하도록 loss function이 학습을 완료하게 됐다. End-to-End Learning으로 이러한 잘못된 sensor input을 생성하거나 controller 출력을 생성하기 힘들지만 중간 수준의 입력단에서는 이것이 가능하다.

시뮬레이션을 통해 증가시킨 data와 loss에 대한 상대적 중요성에 대해 평가한다. 그리고 최종 모델이 실제 세계에서 어떻게 운전하며 상황에 따라 어떻게 행동하는지를 보인다. 중요한점은 강화학습에서 상당 수준의 exploration이 필요한 merging(주변 상황에 대한 높은 수준의 상호작용이 필요하므로)을 잘하는것을 확인하였다. 원래 이러한 행동을 학습하기 위해서는 traffic participants가 필요하였지만 강화학습을 사용하지 않고 오직 offline data를 이용하여 이러한 자율주행을 하여 분야의 경계를 넓혔다는것이 contribution이다.

***

### 2. 연구 동향

1989년에 발표된 ALVINN에 따르면 얕은 Neural Network가 raw camera, laser data를 이용하여 도로를 따라가는 연구를 진행하였다. 그 후 최근들어 다시 이런 End-to-End learning이 부활하였다. 2015년에 Chen에서 발표한 연구에 따르면 CNN이 고속도로에서의 조향에 영향을 끼치는 affordance(앞차와의 거리같은 것)들을 예측하는 연구를 하였다. 2016, 2017년 NVIDIA에서는 CNN이 camera input을 받아 steer를 예측하는 것을 보였다. 2017년 Xu et al.은 discrete하거나 continuous한 action들에 대해 camera 입력을 받아 Neural Network를 이용하여 예측하는 연구를 하였다. 2017년 Keufler et al.은 affordance style의 feature를 이용한 GAIL알고리즘을 사용해 행동에 대한 복제에서 자주 발생하는 cascading error를 해결하여 작은 변화들에 대해 robust하게 학습시켰다. 2018년 Muller et al. 의 연구에 따르면 CARLA 시뮬레이터를 사용하여 Scene segmentation Network를 이용하여 high-level의 control를 출력하는 driving policy를 학습시켜 이걸 이용하여 현실세계에 대한 데이터를 이용해 새로운 real world에 대한 transfer learning을 시켰다. 2016년 Shalev-Shwartz et al. 의 연구에 따르면 강화학습을 이용하여 시뮬레이터상에서 높은 수준의 상호작용이 필요한 merging과 같은 많은 exploration이 필요한 분야에 대한 학습을 진행하였다.

***
### 3. 모델 구조

3.1. Input Output Representation
해당 section에서는 top-down형식의 input에 대해 설명한다. 어떤 시간 t에서 agent에 대한 $$p_{t}, \theta_{t}, s_{t} $$($p_{t}는 자차의 위치 (x_{t}, y_{t}), \theta_{t}는 자차의 heading angle, s_{t}는 속도를 의미한다.$) 이미지에서 현재에 해당하는 자차의 위치 $$p_{0}$$는 언제나 고정되어 있으므로 (u_{0}, v_{0})라고 할 수 있다. data augmentation을 위하여 자차의 $$\theta_{0}$$를 $$\theta_{0} + \Delta$$로 random하게 변환한다. 해당 top-down 이미지는 W X H 로 된 이미지이며 각 필셀당 $\phi$ 미터로 나타낸다. 자차(agent 이후 agent로 표기)가 움직일때, agent가 센서로 바라본 환경또한 움직이므로 볼 수 있는 범위 $$R_{forward} = (H - v_{0})\phi (meter)$$로 나타낼 수 있다. 

<center><img src="/image/model_input.PNG" width="100%" height="100%"></center>

위 Fig1.과 같이 입력은 여러개의 이미지를 받아 한번에 넣는다. 각 input에 대해 설명을 하도록 하겠다.
(a). Roadmap : 3 Channel로 이루어진 렌더링된 이미지로서 차선, 정지 표지판, 커브길, 횡단보도 등 여러 map의 특성들이 담겨진 이미지를 담고 있다.

(b). Traffic lights : grayscale로 렌더링된 이미지로서 과거의 신호등의 상태(빨간, 노랑, 초록)에 대한 정보를 sequential하게 담고 있다. 빨간불에서는 밝은 이미지로 차선의 중앙에 표시하고, 노랑불에서는 중간 수준으로 초록불이나 신호등을 분별할 수 없는 상황에서는 더 어둡게 차선 중앙에 표시하여 신호등 정보를 담았다.

(c). Speed limit : 단일 채널로 렌더링된 이미지로서 차선의 중심에 도로의 속도 제한을 표시했다.

(d). Route : 네이버 지도 및 Google 지도와 같이 생성된 가야하는 경로를 담고있는 이미지이다.

(e). Current agent box : 현재 시간(t = 0)에서의 agent의 full bounding box에 대한 이미지이다.

(f). Dynamic objects in the environment : 주변 차량이나 보행자, 자전거 등 움직이는 동적 물체의 시계열의 oriented box로 렌더링한 이미지이다.

(g). Past agent poses : grayscale로 렌더링된 이미지이며 agent의 이동했던 과거 경로를 하나의 이미지 각 점으로 하여 담았다. 

신호등 정보나 동적 물체는 $$T_{scene}$$초 만큼 sampling되었지만 agent의 과거 위치들은 $$T_{scene}$$보다 긴 $$T_{pose}$$만큼 sampling되었다. 이러한 box로 나타낸 입력데이터는 시뮬레이션 데이터를 통해 생성하거나 실제 센서 데이터를 통하여 만들어 내기 쉽다는 장점이 있다. 이러한 장점은 실제 차에서 돌려보기 전에 closed-loop simulations을 통해 test할 수 있게 했다. 또한, 실제 세계에서는 얻기 힘든 충돌 데이터(실제 충돌을 진행시켜 데이터를 얻기 힘드므로)와 같은 특이한 상황에 대해 접근이 가능하게 했다. 또한 이러한 top-down의 2D 이미지를 사용하는것은 convolutional input에 매우 효율적이게 넣을 수 있고, metadata 및 공간 관계를 사람이 읽을 수 있는 형식으로 표현 가능하다.

만약 I가 위의 설명된 입력들이라고 할때, ChauffeurNet 모델은 Fig.1의 (h)와 같은 output을 출력한다. 해당 식은 다음과 같다. $$p_{t+\delta t} = ChauffeurNet(I, p_{t})$$
이 식에서 $$p_{0}$$는 현재의 agent의 pos값으로 이미 입력으로 알고 있고 ChauffeurNet은 N번의 iteration을 통해 미래 경로인 $${p_{t+\delta t}, p_{t+2\delta t}, ..., p_{t+N\delta t}}를 출력한다. 이러한 경로는 control optimizer로 넘겨져 해당 optimizer가 실제 주행 컨트롤이 가능한 steering 값과 braking 값을 출력한다. 다른 종류의 차종은 같은 주행 경로에 대해서도 다른 control이 필요하지만 해당 Mid-to-Mid Learning을 사용하면 이러한 문제가 발생하지 않게 된다.




+ NPLM, Word2Vec, GloVe, FastText, Swivel 등 2017년 이전에는 단어 수준의 임베딩 기법
    * 단어 수준의 임베딩 기법들은 동음 이의어에 대한 의미 파악 어려움<br/>
<br/>

+ 2018년도 부터 __ELMo__(Embedings from Language Models), **BERT**(Bidirectional Encoder Representations from Transformer), **GPT**(Generative Pre-Training) 등 문장 수준의 임베딩 기법 출현
    * 개별 단어가 아닌 시퀀스로 인해 문맥적 의미 파악 가능
<br/>
+ ELMo, BERT, GPT 모델들은 모두 pretrain과 fine tuning을 통한 임베딩 구현

***
### 4. 임베딩의 종류
1. __행렬 분해 기반 방법__
    + 기존 행렬에 대해 2개 이상의 작은 행렬로 쪼개 분해한 행렬만 사용하거나 더하거나 이어 붙여 사용하는 임베딩 기법

2. __예측 기반 방법__
    + 문장 내에 단어를 없애고 해당 부분에 들어갈 단어를 맞추는 과정을 통해 학습시키는 임베딩 기법

3. __토픽 기반 방법__
    + 주어진 내용에서 latent topic(잠대된 주제)를 추론하는 임베딩 기법
    + __Latent Dirchlet Allocation__(잠재 디리클레 할당)이 디표적인 기법
    + 학습이 끝나면 어떤 주제로 분포되어있는지 확률 벡터로 알 수 있음

***
### 5. 알아두면 좋을 단어
1. __corpus__(말뭉치) : 임베딩 학습이라는 특정 목적을 가지고 수집한 sample
2. __collection__ : corpus에 속한 각각의 집합
3. __sentence__ : 완결된 내용을 나타내는 최소의 독립적인 형식 단위
4. __document(=paragraph)__ : 생각이나 감정, 정보를 공유하는 문장 집합
5. __token__ : word(단어), morpheme(형태소), subword 등으로 부를 수 있으며 하나의 문장은 여러개의 토큰으로 구성
6. __tokenize__ : 문장을 여러개의 토큰 시퀀스로 분석하는 과정
7. __morphological analysis__(형태소 분석) : 문장을 형태소 시퀀스로 나누는 과정
8. __vocabulary__(어휘 집합) : corpus에 있는 모든 document를 sentence로 나누고 tokenize를 한 뒤 중복을 제거한 token들의 집합


