---
title: "임베딩에서의 Vector"

categories:
- 임베딩

tags:
- 임베딩
- 딥러닝
- 자연어
---

***
### 1. 자연어와 Vector의 관계
임베딩을 통해 자연어를 컴퓨터가 처리 가능한 Vector space로 옮겨 컴퓨터가 단어를 이해
이를 위해 임베딩 시 __statistical pattern__(통계적 패턴) 정보를 넣음
>많이 사용 되는 statistical pattern
>1. bag of words : 어떤 단어가 많이 사용되었는지
>2. Language Model : 단어가 어떤 순서로 사용되었는지
>3. Distributional hypothesis :  어떤 단어가 같이 사용되었는지

### 2. Bag of words
단어가 나오는 순서에 관계없이 문서 내 단어의 등장 빈도를 임베딩으로 사용하는 기법
1. Bag이란?
중복 원소를 허용하며 순서를 신경쓰지 않는 집합(multiset)
2. 사용되는 가정
주제가 비슷한 문서에서 나오는 단어 및 빈도수가 비슷하여 Bag of words 임베딩 또한 유사한 경향을 띌것이라 전제
3. 문제점
3.1. 어떤 문서에서든 많이 사용되는 단어가 있을 경우 해당 단어를 통해 문서의 주제 추정 어려움(ex. 을,를,이,가 등)
    - __Term Frequency-Inverse Document Frequency__(TF-IDF)를 사용해 해당 단점 보완
    - TF-IDF(w) = 

