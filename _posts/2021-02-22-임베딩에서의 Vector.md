---
title: "임베딩에서의 Vector"

categories:
- 임베딩

tags:
- 임베딩
- 딥러닝
- 자연어

use_math: true
comments: true
---

***
### 1. 자연어와 Vector의 관계
임베딩을 통해 자연어를 컴퓨터가 처리 가능한 Vector space로 옮겨 컴퓨터가 단어를 이해
이를 위해 임베딩 시 __statistical pattern__(통계적 패턴) 정보를 넣음
>많이 사용 되는 statistical pattern
>1. bag of words : 어떤 단어가 많이 사용되었는지
>2. Language Model : 단어가 어떤 순서로 사용되었는지
>3. Distributional hypothesis :  어떤 단어가 같이 사용되었는지

### 2. Bag of words
단어가 나오는 순서에 관계없이 문서 내 단어의 등장 빈도를 임베딩으로 사용하는 기법
1. Bag이란?
중복 원소를 허용하며 순서를 신경쓰지 않는 집합(multiset)
2. 사용되는 가정
주제가 비슷한 문서에서 나오는 단어 및 빈도수가 비슷하여 Bag of words 임베딩 또한 유사한 경향을 띌것이라 전제
3. 문제점
3.1. 어떤 문서에서든 많이 사용되는 단어가 있을 경우 해당 단어를 통해 문서의 주제 추정 어려움(ex. 을,를,이,가 등)
    - __Term Frequency-Inverse Document Frequency__(TF-IDF)를 사용해 해당 단점 보완
    - $TF-IDF(w)=TF(w)\times log(\frac{N}{DF(w)}) $
    - __TF__ : 단어가 해당 문서에서 나온 빈도수(많이 사용되는 단어가 중요한 단어라는 전제로 사용되는 변수)
    - __IDF__ : 단어가 나온 문서의 수(DF가 클수록 많은 문장에서 사용되는 범용적인 단어, DF가 작을 수록 특이한 단어)
    - TF가 크고 IDF가 작을 수록 해당 주제를 잘 나타내는 단어

### 3. Language Model
단어의 시퀀스에 확률을 부여하는 모델
- 말뭉치에서 단어의 시퀀스가 얼마나 자주 등장하였는지 빈도를 측정
1. n-gram이란?
말뭉치에서 n개의 단어들을 묶어 학습하는 기법
2. 기법 
조건부 확률의 최대 우도 추정법(Maximum Likelihood Estimation)
    - n-gram을 통해 n-1개 단어의 등장 확률로 전체 단어 시퀀스의 등장 확률을 근사시킴
    - Markov assumption(마코프 가정) : 어느 한 state에서의 확률은 그 직전 state에만 의존함
3. 문제점
3.1. Back-Off 방식
    - n-gram 등장 빈도를 n보다 작은 범위의 단어 시퀀스 빈도로 근사
    - 보정 파라미터 $ \alpha, \beta $ 를 사용
    - 5-gram에서 n을 2로한 Back-Off 방식
    - P(안녕하세요 국민대학교 자동차공학과 설재민입니다) $ \approx$ $\alpha$P(자동차공학과 설재민입니다) + $\beta$ 
    - $\alpha$와 $\beta$를 이용해 기존 5-gram에서의 확률값과 2개로 뽑았을때의 확률값의 차이를 보정
3.2 Smoothing 기법
    - 모든 등장 빈도에 K만큼 더하여 높은 빈도수로 나오는 단어들의 전체 확률을 낮추고, 아얘 나오지 않았던 단어가 희박한 확률을 갖게됨
    - K가 1인 스무딩 기법을 라플라스 스무딩이라 정의

### 4. Distributional Hypothesis
1. 분포(distribution)란?
특정 범위(window) 내에 동시에 등장하는 이웃 단어나 context(문맥)의 집합
2. 분포가정(Distributional hypothesis)의 전제
어떤 단어쌍이 비슷한 환경에서 같이 자주 등장한다면 해당 단어의 의미가 유사할 것이라는 전제
3. 분포 가정의 기반
    >"단어의 의미는 곧 그 언어에서의 활용이다(the meaning of a word is its use in the language)"
    (__비트겐슈타인__-1889~1951)
4. 분포 가정 예시
    > 설거지는 물과 세제를 이용하여 한다.
    
    타겟 단어 : 설거지
    문맥 단어(context word) : 물, 세제

    __즉, 타겟단어는 문맥단어와 같이 나올 확률이 높음__
5. 형태소
계열 관계(Paradigmatic relation) : 해당 형태소 자리에 다른 형태소가 대치돼어 사용 가능한지에 대한 관계
__즉, 형태소를 분석한다는것은 계열 관계를 바탕으로 실시__

6. 품사
단어를 문법적 성질의 공통성에 따라 기능(function), 의미(meaning), 형식(form)으로 분류
6.1 기능
                       ㅇㅇㅇ ㅇ.  ㅇ

