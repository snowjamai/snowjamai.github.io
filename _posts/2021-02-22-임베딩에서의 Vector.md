---
title: "임베딩에서의 Vector"

categories:
- 임베딩

tags:
- 임베딩
- 딥러닝
- 자연어

use_math: true
comments: true
---

***
### 1. 자연어와 Vector의 관계
임베딩을 통해 자연어를 컴퓨터가 처리 가능한 Vector space로 옮겨 컴퓨터가 단어를 이해
이를 위해 임베딩 시 __statistical pattern__(통계적 패턴) 정보를 넣음
>많이 사용 되는 statistical pattern
>1. bag of words : 어떤 단어가 많이 사용되었는지
>2. Language Model : 단어가 어떤 순서로 사용되었는지
>3. Distributional hypothesis :  어떤 단어가 같이 사용되었는지

### 2. Bag of words
단어가 나오는 순서에 관계없이 문서 내 단어의 등장 빈도를 임베딩으로 사용하는 기법
1. Bag이란?
중복 원소를 허용하며 순서를 신경쓰지 않는 집합(multiset)
2. 사용되는 가정
주제가 비슷한 문서에서 나오는 단어 및 빈도수가 비슷하여 Bag of words 임베딩 또한 유사한 경향을 띌것이라 전제
3. 문제점
3.1. 어떤 문서에서든 많이 사용되는 단어가 있을 경우 해당 단어를 통해 문서의 주제 추정 어려움(ex. 을,를,이,가 등)
    - __Term Frequency-Inverse Document Frequency__(TF-IDF)를 사용해 해당 단점 보완
    - $TF-IDF(w)=TF(w)\times log(\frac{N}{DF(w)}) $
    - __TF__ : 단어가 해당 문서에서 나온 빈도수(많이 사용되는 단어가 중요한 단어라는 전제로 사용되는 변수)
    - __IDF__ : 단어가 나온 문서의 수(DF가 클수록 많은 문장에서 사용되는 범용적인 단어, DF가 작을 수록 특이한 단어)
    - TF가 크고 IDF가 작을 수록 해당 주제를 잘 나타내는 단어

### 3. Language Model
단어의 시퀀스에 확률을 부여하는 모델
- 말뭉치에서 단어의 시퀀스가 얼마나 자주 등장하였는지 빈도를 측정
1. n-gram이란?
말뭉치에서 n개의 단어들을 묶어 학습하는 기법
2. 기법 
조건부 확률의 최대 우도 추정법(Maximum Likelihood Estimation)
    - n-gram을 통해 n-1개 단어의 등장 확률로 전체 단어 시퀀스의 등장 확률을 근사시킴
    - Markov assumption(마코프 가정) : 어느 한 state에서의 확률은 그 직전 state에만 의존함
3. 문제점
3.1. Back-Off 방식
    - n-gram 등장 빈도를 n보다 작은 범위의 단어 시퀀스 빈도로 근사
    - 보정 파라미터 $ \alpha, \beta $ 를 사용
    - 

