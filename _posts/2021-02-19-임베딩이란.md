---
title: "임베딩이란"

categories:
- 임베딩

tags:
- 임베딩
- 딥러닝
- 자연어
---

***

### 1. 임베딩(embedding)이란?
자연어 처리분야에서 사람의 언어를 벡터로 변환한 결과 및 그 과정                    
단어나 문장을 vector space로 넣는다(embed)에서 유래
  
***

### 2. 임베딩 목적

1. __단어 / 문장 간 관련도 계산__
**Word2Vec** 기법(2013, 구글) : 단어들을 Vector로 바꾸는 기법        
단어의 벡터화를 통한 벡터들 간의 similarity(유사도) 계산
**cosine similarity** : 벡터 간의 코사인 값을 이용한 단어 유사도 계산 방법   
<br/>
2. __transfer learning__
**transfer learning** : 임베딩 된 단어를 다른 딥러닝 모델의 입력으로 사용
<br/>

3. __단어의 의미 및 문법 정보 함축__
**Word Analogy test** : 벡터화 된 단어를 빼고 더함으로서, 단어들 사이의 의미적, 문법적 관계를 도출해 내는 평가 방법 
ex) 사람 - 여자 + 소녀 = 소년 

***
### 3. 문장 단위의 임베딩
+ NPLM, Word2Vec, GloVe, FastText, Swivel 등 2017년 이전에는 단어 수준의 임베딩 기법
    * 단어 수준의 임베딩 기법들은 동음 이의어에 대한 의미 파악 어려움<br/>
<br/>

+ 2018년도 부터 __ELMo__(Embedings from Language Models), **BERT**(Bidirectional Encoder Representations from Transformer), **GPT**(Generative Pre-Training) 등 문장 수준의 임베딩 기법 출현
    * 개별 단어가 아닌 시퀀스로 인해 문맥적 의미 파악 가능
<br/>
+ ELMo, BERT, GPT 모델들은 모두 pretrain과 fine tuning을 통한 임베딩 구현

***
### 4. 임베딩의 종류
1. __행렬 분해 기반 방법__
    + 기존 행렬에 대해 2개 이상의 작은 행렬로 쪼개 분해한 행렬만 사용하거나 더하거나 이어 붙여 사용하는 임베딩 기법

2. __예측 기반 방법__
    + 문장 내에 단어를 없애고 해당 부분에 들어갈 단어를 맞추는 과정을 통해 학습시키는 임베딩 기법

3. __토픽 기반 방법__
    + 주어진 내용에서 latent topic(잠대된 주제)를 추론하는 임베딩 기법
    + __Latent Dirchlet Allocation__(잠재 디리클레 할당)이 디표적인 기법
    + 학습이 끝나면 어떤 주제로 분포되어있는지 확률 벡터로 알 수 있음

***
### 5. 알아두면 좋을 단어
1. __corpus__(말뭉치) : 임베딩 학습이라는 특정 목적을 가지고 수집한 sample
2. __collection__ : corpus에 속한 각각의 집합
3. __sentence__ : 완결된 내용을 나타내는 최소의 독립적인 형식 단위
4. __document(=paragraph)__ : 생각이나 감정, 정보를 공유하는 문장 집합
5. __token__ : word(단어), morpheme(형태소), subword 등으로 부를 수 있으며 하나의 문장은 여러개의 토큰으로 구성
6. __tokenize__ : 문장을 여러개의 토큰 시퀀스로 분석하는 과정
7. __morphological analysis__(형태소 분석) : 문장을 형태소 시퀀스로 나누는 과정
8. __vocabulary__(어휘 집합) : corpus에 있는 모든 document를 sentence로 나누고 tokenize를 한 뒤 중복을 제거한 token들의 집합


